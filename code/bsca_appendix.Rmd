---
title: 'Analysis of multiple specifications'
subtitle: 'Supporting Information'
author: 
  - Christoph Semken
  - David Rossell
date: "`r format(Sys.time(), '%d %B %Y')`"
fontsize: 12pt
indent: true
bibliography: specurveanalysis.bib
link-citations: yes
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    number_sections: yes
    fig_caption: yes
    df_print: kable
header-includes:
   - \usepackage{float,placeins}
   - \usepackage[capitalise]{cleveref}
   - \hypersetup{colorlinks,citecolor=blue,linkcolor=blue,urlcolor=blue}
---

\vspace{2cm} \noindent 
This is the appendix includes extended methods for our paper “Analysis of multiple specifications: statistical validity and a Bayesian proposal”.
In \cref{sec:intro} we briefly review the fundamentals behind Bayesian regression, Bayesian model selection (BMS) and Bayesian model averaging (BMA).
In \cref{sec:simulation}, we compare the statistical properties of the BMA estimator to those of the Specification Curve Analysis (SCA) median permutation test.
The simulation study also shows the needed R code to obtain inference on individual treatment/outcome combinations and on averaged treatment effects.

The code to reproduce our empirical analysis and build a Bayesian Specification Curve Analysis (BSCA), robustness checks and a discussion of analytical choices can be found in our Open Science Framework repository at https://osf.io/m8d4n/.


# Introduction to the Bayesian framework \label{sec:intro}

This section provides a short introduction to the Bayesian regression, BMA and BMA frameworks.  It is designed to allow readers who are unfamiliar with Bayesian statistics to follow the main text. We recommend anyone who wants to use BSCA to first get a deeper understanding of the Bayesian framework. Two excellent introductions are ‘Statistical Rethinking’ by @mcelreath_statistical_2020 and ‘Bayesian Data Analysis’ by @gelman_bayesian_2013. Two more applied texts are Hoeting et al.’s -@hoeting_bma_1999 BMA tutorial and the open textbook (and course) ‘An Introduction to Bayesian Thinking’ by @clyde_introduction_2020.

## Bayesian regression

Bayesian statistics is based on a law of probability, known as Bayes theorem (or Bayes rule).  It states that for two events $A$ and $B$ with non-zero probability, the probability of $A$ occurring given that $B$ occurred is
$$ p(A \mid B) = \frac{p(B \mid A) p(A)}{p(B)}$$
where $p(A)$ and $p(B)$ are the marginal (or unconditional) probabilities of $A$ and $B$ occurring, respectively.

To apply Bayes’ rule to regression models, consider the standard linear regression model $y_i = \beta^T x_i + \alpha^T z_i + \epsilon_i$, where $y_i$ is the value of the dependent variable, $x_i$ contains one or more treatment variables of interest, $z_i$ are adjustment covariates, and $\epsilon_i \sim N(0, \sigma^2)$ are independent errors for observations $i = 1, \ldots, n$. The parameters $(\beta,\alpha,\sigma^2)$ describe the probabilistic dependence of the outcome $y_i$ on the treatment(s) $x_i$, after accounting for the effect of control covariates $z_i$. Specifically, $\beta$ is a vector with one or more parameters that captures the association between the outcome and the treatment(s) of interest. $\alpha$ is a parameter vector capturing the association with control covariates which, despite not being of immediate interest, is needed to reduce biases and variance when estimating $\beta$. 
Although we describe the linear regression setting for simplicity, an analogous construction applies to any other regression model, including the logistic regression model used in the main paper.

Applying Bayes rule, the information about $(\beta,\alpha,\sigma^2)$ after observing the data is contained in a posterior probability distribution that is described by the density function
$$ \overbrace{p(\beta,\alpha,\sigma^2 \mid \text{data})}^{\text{posterior prob.}} = \frac{ \overbrace{p(\text{data} \mid \beta,\alpha,\sigma^2)}^{\text{likelihood}} \overbrace{p(\beta,\alpha,\sigma^2)}^{\text{prior prob.}} }{p(\text{data})} $$
where 'data' denotes the observed data $y_1,x_1,z_1,\ldots,y_n,x_n,z_n$.
Values of $(\beta,\alpha,\sigma^2)$ receiving higher $p(\beta,\alpha,\sigma^2 \mid \text{data})$ are more supported, *a posteriori* after observing the data, than values receiving lower $p(\beta,\alpha,\sigma^2 \mid \text{data})$.

Two important quantities in the above equation are the likelihood function $p(\text{data} \mid \beta,\alpha,\sigma^2)$ and the prior distribution on the parameters $p(\beta,\alpha,\sigma^2)$.  The likelihood is the probability (or probability density, for continuous data) that we would observe our actual data, given the parameters. The denominator $p(\text{data})$ does not depend on $\beta,\alpha,\sigma^2$, it is a normalizing constant that we do not need to calculate directly and follows from the fact that (like all probability density functions) $p(\beta,\alpha,\sigma^2 \mid \text{data})$ integrates to 1.


The posterior information about our treatment effect coefficient(s) of interest $\beta$ given the data is contained in $p(\beta \mid \text{data})$, which can be obtained by integrating the posterior distribution with respect to $\alpha$ and $\sigma^2$, that is
$$ p(\beta \mid \text{data})= \int p(\beta, \alpha, \sigma^2 \mid \text{data}) d\alpha d\sigma^2.$$
The posterior $p(\beta \mid \text{data})$ contains all the probabilistic information needed in a Bayesian analysis to make inference on the treatment effect(s) $\beta$. In particular, one may obtain a point estimate by taking the posterior mean of $p(\beta \mid \text{data})$, and quantify uncertainty via a 95\% posterior interval (an interval that is assigned 95\% probability by $p(\beta \mid \text{data})$).
Of particular importance for BSCA, $p(\beta \mid \text{data})$ can be expressed via BMA as a weighted average across models, as we outline next.

## Bayesian model selection

A common issue in regression analysis – and one that SCA and BSCA try to address – is that there are many potential treatment and control covariates a researcher can choose from.  Specifically, for a total of $p=J+Q$ variables, where $J$ is the number of treatments and $Q$ the number of controls there are $2^p$ models $M_1,\ldots,M_{2^p}$, corresponding to the $2^p$ configurations of the variables in $(x_i,z_i)$ that can potentially be included.  Bayesian model selection (BMS) addresses this issue.  It uses the posterior probability of the model to quantify the evidence in favor of a configuration of covariates being the optimal one [@kass_raftery_bayesfactors_1995]. Here optimal refers to dropping any variable such that its regression coefficient in $\alpha$ is truly equal to 0, while preserving variables that are truly associated with the outcome (i.e. having a non-zero entry in $\alpha$). Two common strategies are then to either report estimates for the model with the highest posterior probability or use it to create weighted estimates. In Bayesian SCA we advocate for the latter, as described in the next section, so that any reported parameter estimates and intervals formally acknowledge the uncertainty in what is the right set of control variables.

The posterior probability of each model is given by Bayes’ rule as
$$ p(M_m~|~\text{data}) = \frac{p(\text{data} \mid M_m)p(M_m)}{p(\text{data})},$$
where $p(M_m)$ is a user-specified prior model probability and $p(\text{data} \mid M_m)$ is the so-called integrated likelihood (or marginal likelihood, or evidence) for model $M_m$. 
It can be computed using standard methods, either via closed-form expressions (when available), via deterministic approximations given by the Bayesian information criterion [BIC; @schwarz_1978], the extended BIC [EBIC; @chen_2008] or Laplace's method [@kass_1990], or via stochastic approximations based on Markov Chain Monte Carlo methods [@friel_2012].

For BSCA, again to avoid contentious prior choices, we use a uniform prior on the model size [the Beta-Binomial(1,1) prior; @scott_betabin_2010].  This prior is uninformative in terms of how many covariates should be included in the regression.  This also makes the estimation of the marginal likelihood computationally easy, since with these priors
$p(\text{data}~|~M_m) p(M_m) \approx e^{-\frac{1}{2} \text{EBIC}_m}$, and hence we may use approximate posterior probabilities
$$ p(M_m | \text{data}) \approx \frac{e^{-\frac{1}{2} \text{EBIC}_m}}{\sum_{m'=1}^{2^p} e^{-\frac{1}{2} \text{EBIC}_{m'}}} $$
where EBIC is the expected Bayesian information criterion [@chen_2008]. Under fairly general conditions, the approximation becomes accurate as the sample size $n$ grows [@schwarz_1978; @rossell_2018 Section 3]. 
As a result, the BSCA model score is just a scaled log-transformation of the EBIC, which is a correction of the widely-used BIC to prevent the inclusion of false positives when the number of treatments $J$ or controls $Q$ are large. 


## Bayesian model averaging

In the Bayesian SCA, we use Bayesian model averaging (BMA) to get a combined estimate for each coefficient of interest from a large number of models, as well as 95\% intervals that consider all possible specifications. 
Specifically, BMA expresses the posterior distribution as the weighted average
$$
p(\beta, \alpha, \sigma^2 \mid \text{data})= \sum_{m=1}^{2^p} p(\beta, \alpha, \sigma^2 \mid M_m, \text{data}) p(M_m \mid \text{data}),
$$
where $p(\beta,\alpha,\sigma^2 \mid M_m,\text{data})$ is the posterior distribution on $(\beta,\alpha,\sigma^2)$ after setting a subset of elements in $\alpha$ to zero, and $p(M_m \mid \text{data})$ is the posterior probability of the model $M_m$.
When the number of models $2^p$ is very large it is unfeasible to conduct the summation above exactly. In such situations one can use Markov Chain Monte Carlo methods, such as Gibbs sampling algorithms implemented in the R packages used in our examples.[^bma-refs]

[^bma-refs]: See also @madigan_bms_1994 for a description of BMS and @hoeting_bma_1999 for a tutorial on BMA.

The point estimates and 95% intervals for treatment effects $\beta$ – obtained from the BMA-weighted posterior $p(\beta \mid \text{data})$ – have several desirable properties.
Under general conditions $p(M_m \mid \text{data})$ converges to 1 (as $n$ grows) for the optimal model that only selects the covariates that are truly associated with the outcome [@dawid_1999]. Under suitable mathematical conditions, this property holds even with large $p$ and or if the data are not exactly generated by the assumed regression model – e.g. due to omitted covariates, non-linear effects, or other potentially incorrect parametric assumptions [@rossell_2018]. 
As a result, the BMA point estimate and 95\% interval converge to the frequentist MLE and 95\% confidence interval obtained under the optimal model [@lecam_asymptotic_1953; @vandervaart_book_1998 Chapter 10, Theorem 10.1].[^proof]

[^proof]: Put differently, it is possible to establish the mathematical validity (from a frequentist statistics viewpoint) of the posterior probabilities $p(M_m \mid \text{data})$.  Briefly, this follows from the fact that $p(M_m \mid \text{data})$ converges to 1 for the optimal model and to 0 for any other model as $n$ grows, Slutsky's theorem and standard Bernstein-von-Mises theorems.

## Inference in BSCA

BSCA uses Bayesian regression and BMA to provide inference (point estimate, 95\% posterior probability interval, and a hypothesis test) for each individual treatment-outcome combination, so that one can fully report their heterogeneity. BSCA can also provide point estimates and hypothesis tests on average treatment effects (ATE). In this section we outline how such inference is obtained.

We first discuss the BSCA for an individual treatment/outcome combination. Let $\beta_j$ be the parameter quantifying said association.
Inference is based on the posterior distribution $p(\beta_j \mid \text{data})$, shown in the BSCA top left panel. The panel also shows the posterior mean $E(\beta_j \mid \text{data})$ as a point estimate and a 95% posterior interval. This posterior distribution is also used to perform a hypothesis test for $\beta_j=0$, specifically when the posterior probability $P(\beta_j \neq 0 \mid \text{data})$ exceeds a threshold one rejects $\beta_j=0$. We recommend the threshold $P(\beta_j \neq 0 \mid \text{data})>0.95$ based on the property that, when the expected value of $P(\beta_j \neq 0 \mid \text{data})$ is above 0.95 (across repeated sampling), then the type I error is guaranteed to be below 0.05 [@rossell_2018 Corollary 1].
Moreover, our EBIC-based formulation guarantees that, if truly an effect $\beta_j=0$ is not present, then the expectation of $P(\beta_j \neq 0 \mid \text{data})$ and the type I error rate converge to 0, as $n$ grows. See Section 1.5 for further discussion on false positive control.

The BSCA top right panel gives point estimates and 95\% intervals given by the posterior distribution $p(\beta \mid M_m,\text{data})$ for each considered model (or the top 100 models, when the number of models $2^p$ is too large). This panel is analogous to standard SCA, except that we focus attention on the models more supported by the data.
The BSCA middle panel gives the model scores, that is the posterior model probabilities $p(M_m \mid \text{data})$, for each configuration of adjustment covariates. Models are sorted decreasingly in terms of their posterior probabilities. The bottom panel mimicks that in SCA and indicates the covariates that correspond to each model.

The BMA estimate and 95\% interval take into account the uncertainty arising from the many possible model specifications, and is asymptotically valid from a frequentist point of view, as explained above. We emphasize that a main motivation for the original SCA was that standard errors conditional on a single selected model fail to account for the model selection uncertainty [@simonsohn_specification_2020]. This issue is naturally resolved in BSCA by using the BMA weights.
We neither have to pick one model (possible resulting in outcome reporting bias) nor take a simple average over all models (many of which maybe relatively implausible given the data, leading to potentially large estimation biases).

In addition, researchers might want to calculate an average treatment effect (ATE). We discuss first reporting an ATE across multiple treatments $\beta_1,\ldots,\beta_J$, for a single outcome. It is common to define the ATE as the mean
$$
\mbox{ATE}= \frac{1}{J} \sum_{j=1}^J \beta_j,
$$
although it is also possible to consider other summaries, such as the median. Given a posterior distribution on the full $\beta$ vector, $p(\beta \mid \text{data})$, one also has an implied posterior distribution $p(\mbox{ATE} \mid \text{data})$. It is hence straightforward to obtain a point estimate for the ATE via the posterior mean $E(\mbox{ATE} \mid \text{data})$, intervals with 95% posterior probability under $p(\mbox{ATE} \mid \text{data})$, and to reject the null hypothesis that ATE=0 when $P(\mbox{ATE} \neq 0 \mid \text{data})> 0.95$. See the next section on how to obtain such inference in R.  As described in the main paper, we recommend always reporting the individual treatment effects – at least their signs – when using the ATE.

Suppose now that one wishes to report a global ATE across $L>1$ outcomes and $J \geq 1$ treatments. Let $\beta_{jl}$ be the regression coefficient associated to treatment $j \in \{1,\ldots,J\}$ and outcome $l= \{1,\ldots,L\}$. SCA defines the ATE as the median $\beta_{jl}$, here we consider the (perhaps more standard) definition based on the mean
$$
\mbox{ATE}= \frac{1}{JL} \sum_{l=1}^L \sum_{j=1}^J \beta_{jl}.
$$

A point estimate for the global ATE is given by the posterior mean
$$
E(\mbox{ATE} \mid \mbox{data}) = \frac{1}{JL} \sum_{l=1}^L \sum_{j=1}^J E( \beta_{jl} \mid \mbox{data}).
$$

<!-- Regarding Bayesian inference on the global ATE across outcomes, the most comprehensive solution would be to formulate a multivariate regression model for the $L$ outcomes, such that the dependence between outcomes is captured. However, such multivariate models require a more involved implementation and computation that is outside the scope of this short note.  -->
Note that for linear regression the global ATE associated to regressing each individual outcome on treatment and controls is mathematically equivalent to the ATE associated to regressing the average outcome on the treatments and controls. 
The simulation study in our appendix illustrates how to exploit this property.
<!-- see Lemma 1 below for a precise statement and proof and the simulation study in Section 2 for an example.-->

An important remark, which makes us caution against using the global ATE for statistical inference, is that it assigns equal weight to all outcomes. This may be inappropriate in situations where some of the outcomes are strongly correlated. For instance, suppose that there are $J=10$ outcomes, 9 of which measure a very similar latent quantity (they are similar items within a questionnaire) whereas the tenth outcome measures an inherently different quantity. The global ATE will be mostly determined by outcomes 1-9, whereas intuitively one might want to discount their weight. Given that defining alternative global ATE's is a potentially contentious issue, in our examples we use the standard ATE definition, and recommend that BSCA users rely on outcome-specific results.


<!--
**{\bf Lemma 1.}** 
*Let $y_i$ be an $L$-dimensional response, $x_i$ a $J$-dimensional treatment, and $z_i$ a $q$-dimensional control variable vector for individuals $i=1,\ldots,n$. Consider the regression*
$$
y_i= \beta^T x_i + \alpha^T z_i + \epsilon_i,
$$
*where $\epsilon_i$ is an $L$-dimensional error vector, $\beta$ is an $L \times J$ treatment effect matrix and $\alpha$ an $L \times q$ matrix with regression parameters for the controls.*

*Let $m_i= \sum_{l=1}^L y_{il}/L$ be the mean of $y_i$ and consider the regression model*
$$
m_i= \eta^T x_i + \theta^T z_i + \xi_i,
$$
*where $\xi_i \in \mathbb{R}$ is the error term. Then it holds that*
$$
\mbox{ATE}= \sum_{l=1}^L \sum_{j=1}^J \frac{\beta_{jl}}{JL}= \sum_{j=1}^J \frac{\eta_j}{J}.
$$

**{\bf Proof.}**
*First, note that the ATE can be written as*
$$
\mbox{ATE}= \sum_{l=1}^L \sum_{j=1}^J \frac{\beta_{jl}}{JL}= \frac{{\mathbb 1}_J^T}{J}  \beta \frac{{\mathbb 1}_L}{L} 
$$
*where ${\mathbb 1}_J=(1,\ldots,1)$ is the $J$-dimensional one-vector, and ${\mathbb 1}_L$ the $L$-dimensional one-vector. Since $m_i= \sum_{l=1}^L y_{il}/L= {\mathbb 1}_L^T y_i/L$, it follows that*
$$
m_i= \frac{{\mathbb 1}_L^T}{L} \beta^T x_i + \frac{{\mathbb 1}_L^T}{L} \alpha^T z_i + \frac{{\mathbb 1}_L^T}{L} \epsilon_i=
\eta^T x_i + \theta^T z_i + \xi_i,
$$
*where $\eta^T= {\mathbb 1}_L^T \beta^T/L$, $\theta^T= {\mathbb 1}_L^T \alpha^T/L$ and $\xi_i= {\mathbb 1}_L^T \epsilon_i/L$. Hence*
$$
\mbox{ATE}= \frac{{\mathbb 1}_J^T}{J}  \beta \frac{{\mathbb 1}_L}{L}= \frac{{\mathbb 1}_J^T}{J} \eta= \sum_{j=1}^J \frac{\eta_j}{J},
$$
*as we wished to prove.*
-->



## Statistical considerations and false positive control

As discussed, our EBIC-based formulation guarantees that, as the sample size $n$ grows, the total number of false discoveries across all tested treatment-outcome combinations converges to zero. See the simulation study in the next section for an empirical assessment of this property.
In this section we discuss alternative Bayesian and non-Bayesian strategies to control false positives such as P-value adjustment and the False Discovery Rate.

Regarding alternative strategies, one could consider other Bayesian formulations that set different priors and still attain good properties (e.g. model selection consistency) as the sample size $n$ grows. For instance, one could set so-called Complexity priors on the model space [@castillo_2015] or non-local priors on the regression coefficients [@johnson_2010; @johnson_2012; @rossell_2017]. These are recent developments in the statistical literature to further improve the prevention of false positives in settings with many parameters or hypothesis tests, but given the large sample size in our examples we found that these refinements were not needed.
It is also possible to refine our formulation for situations where one considers many outcomes. Briefly, the discussed Beta-Binomial/EBIC property that the probability of having any false positive (family-wise error rate, FWER) converges to 0 for large $n$, for each individual outcome, implies that the overall FWER across outcomes also converges to 0. In our experience the Beta-Binomial/EBIC in practice attains a very good FWER control, as long as the number of outcomes is moderate relative to $n$. In situations with a truly large number $L$ of outcomes one can extend the Beta-Binomial prior, by setting uniform prior probabilities to models that select $0,1,\ldots,(J+q) L$ variables across all outcomes, where $q$ is the number of potential control variables. While these refinements are potentially interesting, we found them to be unnecessary in the considered applications.

We remark that one can also use non-Bayesian false positive control methods. 
First, note that BMA could be viewed simply as a mechanism to obtain a point estimate, both a global $\widehat{\mbox{ATE}}$ and $\hat{\beta}_{jl}= E(\beta_{jl} \mid \text{data})$ for individual parameters. One can then use a permutation test akin to that used in SCA to obtain a P-value for the ATE. Permutation tests can also provide P-values for individual parameters, and one could use standard P-value adjustment or False Discovery Rate control methods to prevent false positive inflation due to testing multiple $\beta_j$'s. See @benjamini_1995 and @efron_2007 for discussion on FWER and FDR control. Briefly, these methods ensure that the probability or proportion of false positives is below a pre-specified threshold, the default being the usual 0.05. That is, these methods are willing to admit a fixed positive probability of including regression parameters that are truly zero (similar to a P-value for a single test admitting a 0.05, say, false positive probability). In contrast the Beta-Binomial/EBIC formulation ensures that said probability converges to 0, that is as $n$ grows one discards all truly irrelevant parameters, which intuitively provides a stronger false positive control.




# SCA and BSCA estimator and hypothesis testing properties: a simulation study \label{sec:simulation}

```{r child = 'bma_simulation.Rmd'}
```




\FloatBarrier
# SI References {.unlisted .unnumbered}
<!-- bibliography inserted here -->