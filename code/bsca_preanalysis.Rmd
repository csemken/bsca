---
title: 'Specification analysis for technology use and teenager well-being'
subtitle: 'Data preparation and pre-analysis'
author: 
  - Christoph Semken
  - David Rossell
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: kable
---

```{r setup, message=FALSE}

library(tools)
library(mombf)
library(BAS)
library(tidyverse)
library(stargazer)
library(cowplot)
library(gridGraphics)

PATH = Sys.getenv("BSCA_PATH")
NITER_YRBS = 10; NITER_MCS = 10

source(file.path(PATH,'code/functions.R'))

supplement = list()  # save supplemental figures
plot_path = file.path(PATH, 'data/export/plots')

```


In this file we pre-process the data, perform a preliminary analysis and create additional results for our main paper “Bayesian Specification Curve Analysis”.
Section 1 covers the Youth Risk Behavior Survey (YBRS) dataset, whereas Section 2 covers the Millenium Cohort Study (MCS) dataset. We start with the dataset produced by Orben & Przybylski’s (OP), see the main paper for a reference.  The first subsection of each section explains in detail how to reproduce our analysis using the original data, their code and ours.  The remaining sections perform the pre- and auxiliary analyses.

To reproduce this file, first set the `PATH` variable to the project root (folder above the one containing this file). Next, follow the instructions in the two “Generation of data” subsections to prepare the data and compare the MD5 checksums to those in the compiled version of this file (`bcsa_preanalysis.html`). Finally, knit this file using rmarkdown and the software versions specified in the `README.md`.


# YRBS DATA

## DATA PRE-TREATMENT

In this section, we import and treat the data, as well as select relevant variables.

### Generation of data

To be able to run this analysis, download the YRBS data, convert it into CSV, save both files to `data/yrbs`, generate the OP YRBS dataset and save it to `data/op_export/1_1_prep_yrbs_data.csv`.  

We downloaded the latest YRBS data in the MDB format from the [CDC](https://www.cdc.gov/healthyyouth/data/yrbs/data.htm) on 22 October 2019.  We then converted it into a CSV file using Microsoft Access 2019.  The files have the following MD5 checksums: 

```{r}
files = c(
  file.path(PATH, 'data/yrbs/yrbs_SADC_2017_National.MDB'), 
  file.path(PATH, 'data/yrbs/yrbs_SADC_2017_National.csv')
)
md5s = md5sum(files)
names(md5s) = basename(files)
md5s
```

We then ran the script `1_1_prep_yrbs.R` from [OP’s replication files](https://osf.io/e84xu/), after adjusting the input and output file paths.  The resulting dataset has the following MD5 checksum:

```{r}
yrbs_path = file.path(PATH,'data/op_export/1_1_prep_yrbs_data.csv')
md5s = md5sum(yrbs_path)
names(md5s) = basename(yrbs_path)
md5s
```

### Import data

Import the data, as generated by OP script. Remove constant variables.

```{r}
data= read.csv(yrbs_path,header=TRUE)
data= data[, !(colnames(data) %in% c('sitecode','sitename','sitetype','sitetypenum'))]
```

### Treatment variables 

```{r}
x_vars_op= c("q81_n", "q82_n", "tech")
x_names_op= c("TV Use", "Electronic Device Use", "Mean Technology Use")
x= data[,x_vars_op]; names(x)= x_names_op
```

### Outcome variables

Turn the outcome variables into what they say: 0=no, 1=yes.

```{r}
y_vars_op= c("q26_n", "q27_n", "q28_n", "q29_nd", "q30_nd")
y_names= c("loneliness", "think suicide", "plan suicide", "commit suicide", "doctor suicide")
for (v in y_vars_op) {data[[v]] = 1 - data[[v]]}
y= data[,y_vars_op]; names(y)= y_names

```

### Control variables

Indicate controls used in YRBS study (adolescent's race) and additional controls. Discretize BMI according to the [standard definition](https://www.cdc.gov/obesity/adult/defining.html).

```{r}
cvars= c("race_di")
c_names= c("dichotomous race")

data$bmir= cut(data[,'bmi'], breaks=c(0,18.5,25,30,Inf)) #discretize body mass index
levels(data$bmir)= c('underw','normal','overw','obese')
cvarsplus= c("age_n", "sex_n", "grade_n", "year", "bmir")  #survyear same as year
```

### Re-coding variables

Based on the below analyses we make the following adjustments to the variables, as coded and used by OP.

TV usage has a non-monotonic (U-shaped) treatment effect on all outcomes. Discretize the variable into low, medium and high usage, using cut-offs revealed by the linearity analysis.

```{r}
data$q81_nr= cut(data[,'q81_n'], breaks=c(0,2,6,Inf)) #discretize tv use
levels(data$q81_nr)= c('low', 'medium', 'high')
```

Normalize the ED usage values to [0, 1], which makes the coefficient more easily interpretable (difference between min and max usage).

```{r}
data$q82_nr = (data$q82_n - 1)/6
```

Mean technology use is a linear combination of TV and ED use. Leave it out of the regression.

```{r}
x_vars= c("q81_nr", "q82_nr")
x_names= c("TV Use", "Electronic Device Use")
```

Age 12 and 13 have very low number of observations (are very young for grades 9-12).  They also have different effects, than age in general (which appears roughly linear).  Include them in the regression to control for differential effects at these ages.

```{r}
data$age12 = data$age_n == 12
data$age13 = data$age_n == 13
for (v in  c('age12', 'age13')) { data[,v]= as.numeric(data[,v]) }
```

Set commit and doctor to 1 (did not attempt suicide and did not see a doctor about it) if both think and plan are 1 (did not think or plan suicide) to solve a problem with missing values (see below).

```{r}
y_vars = y_vars_op= c("q26_n", "q27_n", "q28_n", "q29_ndr", "q30_ndr")
data$q29_ndr = data$q29_nd
data$q30_ndr = data$q30_nd

cond= replace_na(data[y_vars[2]] == 1 & data[,y_vars[3]] == 1, FALSE)
data[cond, y_vars[4:5]]= 1
y= data[,y_vars]; names(y)= y_names
```


## EXPLORATORY DATA ANALYSIS

Format categorical variables as factors.

```{r}
xf= x
dataf= data

nn= c('TV Use','Electronic Device Use')
for (i in 1:length(nn)) xf[,nn[i]]= factor(xf[,nn[i]])
nn= c('age_n','sex_n','grade_n','year')
for (i in 1:length(nn)) dataf[,nn[i]]= factor(dataf[,nn[i]])
```


### Value counts

Outcomes.

```{r}
apply(y,2,'table')
```

Treatments.

1 = No usage
2 = Less than 1 hour per day 
3 = 1 hour per day 
4 = 2 hours per day 
5 = 3 hours per day 
6 = 4 hours per day
7 = 5 or more hours per day

```{r}
apply(xf,2,'table')  #table for the treatments
```

Controls.

```{r}
apply(dataf[,c(cvars,cvarsplus)],2,'table')  #table for the controls
```

### Missing values

There are quite a few NA's for planning/committing suicide, maybe not everyone was asked these questions.

```{r}
colSums(is.na(y))
```


This is not an issue with the year; the question was asked in every wave.

```{r}
dataf[, c('year', 'q29_nd')] %>% group_by(year) %>% summarise(commit_na = sum(is.na(q29_nd)))
```

Most of the participants who did not answer these questions said they had not thought about or planned a suicide.  They may have just skipped the next two questions (about commiting suicide and being treated by a doctor for a suicide attempt).

```{r}
dataf[, c('q27_n', 'q28_n', 'q29_nd', 'q30_nd')] %>% rename(think= q27_n, plan = q28_n) %>% group_by(think, plan) %>% 
  summarise(commit_na = sum(is.na(q29_nd)), doctor_na = sum(is.na(q30_nd)))

```


```{r}
ywithNA= y
for (i in 1:ncol(ywithNA)) { z= as.character(ywithNA[,i]); z[is.na(z)]= "NA"; ywithNA[,i]= factor(z) }
table(ywithNA[,'plan suicide'], ywithNA[,'commit suicide'])
```

We solve this by setting commit and doctor to 1 (did not attempt suicide and did not see a doctor about it) if both think and plan are 1 (did not think or plan suicide) above. This solves the NA puzzle.

```{r}
colSums(is.na(data[,y_vars]))
```

### Correlations

Treatments. Note: Mean use is a linear combination of TV and ED use.

```{r}
xnum= apply(xf,2,as.numeric)
round(cor(xnum,use='pairwise.complete.obs'),3)
```

Outcomes.

```{r}
round(cor(y,use='pairwise.complete.obs'),3)
```

Controls. Correlation between age and grade is high but not perfect, it may be possible to disentangle the effect of each on wellbeing.

```{r}
datanum= sapply(c(cvars,cvarsplus), function(z) as.numeric(dataf[,z]))
round(cor(datanum, use='pairwise.complete.obs'),3)
```

Bivariate pmf to explore the dependence between TV and electronic device use.

```{r}
ypairs= combn(ncol(y),2)
tab= lapply(1:ncol(ypairs), function(i) table(y[,ypairs[1,i]],y[,ypairs[2,i]]))
names(tab)= sapply(1:ncol(ypairs), function(i) paste(names(y)[ypairs[,i]], collapse=','))
plotxtab(xf[,1], xf[,2], xlab=names(xf)[1], ylab= names(xf)[2])
```

Focus on the two extreme and most frequent categories of electronic device use.

```{r}
tab= t(table(xf[,1],xf[,2])[,c(1,6)]); tab= 100 * tab/rowSums(tab)  #ISSUE: slight negative association between TV & electronic device use
rownames(tab)= paste(names(xf)[2], '=',c(1,6))
barplot(tab, xlab=names(xf)[1], beside=TRUE, ylab='% adolescents', legend=TRUE)
```


## LINEARITY OF TREATMENT AND COVARIATE EFFECTS

### Outcome 1. Loneliness


```{r}

idy= 1; idx= c(1,2)
datareg= data.frame(y[,idy], xf[,idx], dataf[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

```

Fit logistic regression model via MLE. Higher TV use associated to less loneliness, higher ED use to more loneliness. The effect of age is mainly from 12 to $\geq$13 years old. There's higher loneliness at higher grades. Sex has a clear effect. Higher BMI associated to less loneliness (not intuitive why this might be the case).

```{r}

fit1= glm(y ~ ., data=datareg, family=binomial(link='logit'))
bmle= getci(fit1)
summary(fit1)

```

Graphically. It looks like treatment effects can roughly be captured as linear for ED, but not for TV use (which has a U-shaped effect). BMI, age, grade etc also appear fairly linear.

```{r}
variables = c(names(x)[idx], cvarsplus)[-4]
plots = list()
for (v in variables) {
  sel= grep(v,rownames(bmle))
  plot(bmle[sel,1],ylim=range(bmle[sel,]), xaxt='n')
  segments(x0=1:length(sel),y0=bmle[sel,2],y1=bmle[sel,3])
  axis(side=1, at=1:length(sel), labels=rownames(bmle)[sel])
  title(v)
}
```


### Outcome 2. Think suicide

```{r}
idy= 2; idx= c(1,2)
datareg= data.frame(y[,idy], xf[,idx], dataf[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]

fit1= glm(y ~ ., data=datareg, family=binomial(link='logit'))
bmle= getci(fit1)
summary(fit1)
```

```{r, fig.height=3, fig.width=7}
variables = c(names(x)[idx], cvarsplus)[-4]
plots = list()
for (v in variables) {
  sel= grep(v,rownames(bmle))
  plot(bmle[sel,1],ylim=range(bmle[sel,]), xaxt='n', ylab='Estimated coefficient')
  segments(x0=1:length(sel),y0=bmle[sel,2],y1=bmle[sel,3])
  axis(side=1, at=1:length(sel), labels=rownames(bmle)[sel])
  title(v)
  if (v == 'TV Use') {
    supplement$lin_tv_think = recordPlot()
  }
}

```

### Outcome 3. Plan suicide

```{r}
idy= 3; idx= c(1,2)
datareg= data.frame(y[,idy], xf[,idx], dataf[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]

fit1= glm(y ~ ., data=datareg, family=binomial(link='logit'))
bmle= getci(fit1)
summary(fit1)
```

```{r}
variables = c(names(x)[idx], cvarsplus)[-4]
plots = list()
for (v in variables) {
  sel= grep(v,rownames(bmle))
  plot(bmle[sel,1],ylim=range(bmle[sel,]), xaxt='n')
  segments(x0=1:length(sel),y0=bmle[sel,2],y1=bmle[sel,3])
  axis(side=1, at=1:length(sel), labels=rownames(bmle)[sel])
  title(v)
}

```

### Outcome 4. Commit suicide

```{r}
idy= 4; idx= c(1,2)
datareg= data.frame(y[,idy], xf[,idx], dataf[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]

fit1= glm(y ~ ., data=datareg, family=binomial(link='logit'))
bmle= getci(fit1)
summary(fit1)
```

Including age 13 leads to a very large (imprecisely estimated) coefficient in this regression.

```{r}
variables = c(names(x)[idx], cvarsplus)[-4]
plots = list()
for (v in variables) {
  sel= grep(v,rownames(bmle))
  if (v == 'age_n') sel=sel[-1]
  plot(bmle[sel,1],ylim=range(bmle[sel,]), xaxt='n')
  segments(x0=1:length(sel),y0=bmle[sel,2],y1=bmle[sel,3])
  axis(side=1, at=1:length(sel), labels=rownames(bmle)[sel])
  title(v)
}

```

### Outcome 5. Doctor suicide

```{r}
idy= 5; idx= c(1,2)
datareg= data.frame(y[,idy], xf[,idx], dataf[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]

fit1= glm(y ~ ., data=datareg, family=binomial(link='logit'))
bmle= getci(fit1)
summary(fit1)
```

```{r}
variables = c(names(x)[idx], cvarsplus)[-4]
plots = list()
for (v in variables) {
  sel= grep(v,rownames(bmle))
  if (v == 'age_n') sel=sel[-1]
  plot(bmle[sel,1],ylim=range(bmle[sel,]), xaxt='n')
  segments(x0=1:length(sel),y0=bmle[sel,2],y1=bmle[sel,3])
  axis(side=1, at=1:length(sel), labels=rownames(bmle)[sel])
  title(v)
}

```



## REGRESSION ANALYSIS

Issues with Orben's et al analysis

1. Orben et al run linear regression, but all 5 outcomes are discrete

2. Sex and grade highly significant, but ignored in OP's analysis


### Graphical parameters


Use the re-defined treatment variables (see data pre-treatment).

```{r}
x= data[,x_vars]; names(x)= x_names
```


Include one year coefficient in the control variable panel and set readable names for treatments and controls.  Include year dummies but only show them as one variable in the BSCA.  Include dummies for ages 12 and 13, to control for effects of these students being misaligned with their grade.

```{r}
cvars= "race_di"
cvarsplus= c("age12", "age13", "age_n", "sex_n", "grade_n", "year", "bmi")
c_names = c('Race', 'Aged 12', 'Aged 13', 'Age', 'Male', 'Grade', 'Year', 'BMI')

data[,'year']= factor(data[,'year'])  # keep year as a factor
id_years=c(12:14)  # hide all but one year coefficient (same variable -> included together)

x_labels = c('TV use: medium', 'TV use: high', 'Electronic device use')
var_labels = c_names
```

Transform the y labels into the odds ratio (from min=0 to max=1), by exponentiating the coefficient. 

```{r}
y_labels = seq(0.8, 2, 0.2); names(y_labels) = log(y_labels)
```

Only show 50 models and decrease the size of the legend to nicely combine in the grid figure.

```{r}
maxmodels = 50
legend_size = 1
```


### Outcome 1. Loneliness

Regress the first outcome (loneliness) on the two treatments. Store the data in `datareg`.

```{r}

idy= 1; idx= c(1,2)
datareg= data.frame(y[,idy], x[,idx], data[,c(cvars,cvarsplus)])
names(datareg) = c('y', names(x)[idx], c_names)
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

```

Fit logistic regression model via MLE. Higher TV use associated to less loneliness, higher ED use to more loneliness. The effect of age is mainly from 12 to $\geq$13 years old. There's higher loneliness at higher grades. Sex has a clear effect. Higher BMI associated to less loneliness (not intuitive why this might be the case).

```{r}

reg= y ~ . 
fit1= glm(reg, data=datareg, family=binomial(link='logit'))
bmle= getci(fit1)
summary(fit1)

```


Run EBIC-based model selection. This is an approximation to Bayesian model selection under the unit information prior, and BetaBin(1,1) on the model space. Unfortunately for logistic regression this uses a slowish R implementation. We set `includevars=1` to force the inclusion of the intercept.

```{r}

ms= mombf:::modelSelectionGLM(reg, data=datareg, includevars=1, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1))

```

Most posterior probability is assigned to the 2 top models, i.e. there's little uncertainty on what variables should be included.

```{r}

pp= postProb(ms)
head(pp)

```

We obtain BMA estimates, 95% posterior intervals and marginal posterior inclusion probabilities. Both TV and ED use have very high posterior prob.  Similarly strong evidence for including sex, grade and bmi. Small post prob to race, the adjustment variable considered by Orben et al. Age and year are dropped.


```{r}

b= coef(ms)
b

```

Odds-ratios help characterize the practical importance of the estimated treatment effects (use `getOR` defined in functions.R for convenience).

```{r}

getOR(b, treat='TV Use') %>% format(digits=2, scientific=FALSE) # medium and high treatments
getOR(b, treat='Electronic', treatvals=round((1:6)/6, 2)) %>% format(digits=3, scientific=FALSE) #increasing treatment by 1,...,6 units

```

We could obtain parameter estimates and posterior intervals for the top 100 models.

```{r, eval=FALSE}

bmodels= coefByModel(ms, maxmodels=100, alpha=0.05)

sel= "`Electronic Device Use`"
bsel= cbind(postmean=bmodels$postmean[,sel],ci.low=bmodels$ci.low[,sel],ci.up=bmodels$ci.up[,sel])
head(round(bsel,3))

```


Produce the BMA SCA plots. Clearly TV and ED use have effects of opposing signs.

```{r, fig.height=3, fig.width=4}

idx_fit=c(2:4) 
single_bsca(ms, coefidx=idx_fit, omitvars=c(1, idx_fit, id_years), x.labels=x_labels, var.labels=var_labels, y.labels=y_labels, maxmodels=maxmodels, legend.cex=legend_size)

# save the plot
bscas = list()
bscas[[idy]] = recordPlot()

```



### Outcome 2. Think suicide


```{r, fig.height=3, fig.width=4}

idy= 2; idx= c(1,2); idx_fit=c(2:4) 
reg= y ~ . 

datareg= data.frame(y[,idy], x[,idx], data[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

ms= mombf:::modelSelectionGLM(reg, data=datareg, includevars=1, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1))
b= coef(ms)

getOR(b, treat='TV Use') %>% format(digits=2, scientific=FALSE) # medium and high treatments
getOR(b, treat='Electronic', treatvals=round((1:6)/6, 2)) %>% format(digits=3, scientific=FALSE) #increasing treatment by 1,...,6 units

single_bsca(ms, coefidx=idx_fit, omitvars=c(1, idx_fit, id_years), x.labels=x_labels, var.labels=var_labels, y.labels=y_labels, maxmodels=maxmodels, legend.cex=legend_size)
bscas[[idy]] = recordPlot()
```


### Outcome 3. Plan suicide


```{r, fig.height=3, fig.width=4}

idy= 3; idx= c(1,2); idx_fit=c(2:4) 
reg= y ~ . 

datareg= data.frame(y[,idy], x[,idx], data[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

ms= mombf:::modelSelectionGLM(reg, data=datareg, includevars=1, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1))
b= coef(ms)

getOR(b, treat='TV Use') %>% format(digits=2, scientific=FALSE) # medium and high treatments
getOR(b, treat='Electronic', treatvals=round((1:6)/6, 2)) %>% format(digits=3, scientific=FALSE) #increasing treatment by 1,...,6 units

single_bsca(ms, coefidx=idx_fit, omitvars=c(1, idx_fit, id_years), x.labels=x_labels, var.labels=var_labels, y.labels=y_labels, maxmodels=maxmodels, legend.cex=legend_size)
bscas[[idy]] = recordPlot()

```


### Outcome 4. Commit suicide


```{r, fig.height=3, fig.width=4}

idy= 4; idx= c(1,2); idx_fit=c(2:4) 
reg= y ~ . 

datareg= data.frame(y[,idy], x[,idx], data[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

ms= mombf:::modelSelectionGLM(reg, data=datareg, includevars=1, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1))
b= coef(ms)

getOR(b, treat='TV Use') %>% format(digits=2, scientific=FALSE) # medium and high treatments
getOR(b, treat='Electronic', treatvals=round((1:6)/6, 2)) %>% format(digits=3, scientific=FALSE) #increasing treatment by 1,...,6 units

single_bsca(ms, coefidx=idx_fit, omitvars=c(1, idx_fit, id_years), x.labels=x_labels, var.labels=var_labels, y.labels=y_labels, maxmodels=maxmodels, legend.cex=legend_size)
bscas[[idy]] = recordPlot()

```


### Outcome 5. Doctor suicide


```{r, fig.height=3, fig.width=4}

idy= 5; idx= c(1,2); idx_fit=c(2:4) 
reg= y ~ . 

datareg= data.frame(y[,idy], x[,idx], data[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

ms= mombf:::modelSelectionGLM(reg, data=datareg, includevars=1, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1))
b= coef(ms)

getOR(b, treat='TV Use') %>% format(digits=2, scientific=FALSE) # medium and high treatments
getOR(b, treat='Electronic', treatvals=round((1:6)/6, 2)) %>% format(digits=3, scientific=FALSE) #increasing treatment by 1,...,6 units

single_bsca(ms, coefidx=idx_fit, omitvars=c(1, idx_fit, id_years), x.labels=x_labels, var.labels=var_labels, y.labels=y_labels, maxmodels=maxmodels, legend.cex=legend_size)
bscas[[idy]] = recordPlot()

```


### Combine figures

Combine all outcomes not featured in the main analysis into a single graph shown in the supplement.

```{r, fig.height=6, fig.width=8}
supplement$bsca_other = plot_grid(plotlist=bscas[-2], ncol=2, labels='auto')
ggsave(
  file.path(plot_path, 'yrbs_bsca_other.png'), 
  supplement$bsca_other,
  height=6, width=8
)
supplement$bsca_other
```


## ROBUSTNESS CHECKS

### Only one TV coefficient

```{r, fig.height=4, fig.width=7}
data$q81_nra = (data$q81_n - 1)/6

idy= 2; idx= c(1,2); idx_fit_num=c(2:3) 
x_labels_num = c('TV use', 'Electronic device use')
reg= y ~ . 

datareg= data.frame(y[,idy], data[,c('q81_nra', 'q82_nr')], data[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

ms= mombf:::modelSelectionGLM(reg, data=datareg, includevars=1, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1))

single_bsca(ms, coefidx=idx_fit_num, omitvars=c(1, idx_fit_num, id_years), x.labels=x_labels_num, var.labels=var_labels, y.labels=y_labels)
supplement$bsca_tv_num = recordPlot()
```

### Linear regression

Not implemented in mombf, use BAS. 

```{r, fig.height=4, fig.width=7}
# regression parameters
idy= 2; idx= c(1,2); idx_fit=c(2:4) 
reg= y ~ . 
datareg= data.frame(y[,idy], x[,idx], data[,c(cvars,cvarsplus)])
names(datareg)[1]= 'y'
names(datareg)[2:(1+length(idx))]= names(x)[idx]
sel= rowSums(is.na(datareg))==0
datareg= datareg[sel,]

# regression 
ms = BAS:::bas.lm(
  reg, data=datareg,
  prior='BIC', modelprior=beta.binomial()
)

# bsca
single_bsca(ms, coefidx=idx_fit, omitvars=c(1, idx_fit, id_years), x.labels=x_labels, var.labels=var_labels)
supplement$bsca_lin = recordPlot()
```


## SUBGROUP ANALYSIS

We perform a gender subgroup analysis. The variables are parameterized as described in the paper. Interaction terms are included only if the corresponding main variable is included. BMA is done through Gibbs sampling.

```{r}
idx=c(1,2); idg="Male"
{
  options(na.action='na.pass')
  x.reg = model.matrix(~ ., x[,idx])[, -1]
  colnames(x.reg) = x_labels
}
ms_inter = list(); coef_inter = list()

for (idy in 1:length(y_vars)) {
  yname = y_names[idy]
  message(yname)
  
  datareg = na.omit(data.frame(y[,idy], x.reg, data[,c(cvars,cvarsplus)]))
  names(datareg) = c('y', colnames(x.reg), c_names)
  
  # treatments
  idt = colnames(x.reg)
  datareg[idt] = datareg[idt] - 1/2
  # group
  g = datareg[[idg]]; rho = sum(g == 1)/length(g)
  datareg[[idg]] = ifelse(g == 1, 1-rho, -rho)
  # interaction 
  inter = datareg[idt]*datareg[[idg]]
  colnames(inter) = paste0(colnames(inter), " × male")
  
  d = mombf:::createDesign(y ~ ., cbind(datareg, inter))
  p=length(d$constraints); pi=ncol(inter)
  d$constraints[(p-pi+1):p] = 1:pi+1
  ms_inter[[yname]] = mombf:::modelSelection(
    d$y, d$x, includevars=1, family="binomial", 
    priorDelta=modelbbprior(1,1), priorCoef=bicprior(), priorGroup=bicprior(),
    niter=NITER_YRBS, groups=d$groups, constraints=d$constraints,
  )
  coef_inter[[yname]] = coef(ms_inter[[yname]])
}
```

We can never rule out a zero gender effect.

```{r}
supplement$mbsca_inter = multi_bsca(
  coef_inter, ms=ms_inter, conversion=exp,  y.scale='Odds ratio', 
  treatments=tail(rownames(coef_inter$loneliness), n=length(x_labels)),
)
supplement$mbsca_inter
```



## SAVE DATA

Save the data for use in `bsca_main.Rmd`.

```{r}
save(
  data, x_vars, x_names, x_labels, x, y_vars, y_names, y, cvars, cvarsplus, c_names, supplement,
  file=file.path(PATH, 'data/export/yrbs.Rdata')
)
```





# MCS DATA

```{r mcs-setup, message=FALSE}
# clean up the R environment
rm(list=setdiff(ls(), c('PATH', 'plot_path', 'NITER_MCS')))
source(file.path(PATH,'code/functions.R'))
supplement = list()  # save supplemental figures
```

## DATA PRE-TREATMENT

In this section, we import and treat the data, as well as select relevant variables.

### Generation of data

To be able to run this analysis, download the MCS6 data to `data/mcs/tab`, generate the OP MCS dataset and save it to `data/op_export/1_3_prep_mcs_data.csv`.  

We downloaded the MCS6 data (4th edition) from the [UK data service](https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id=8156) on 22 October 2019 in the CSV format.  The files have the following MD5 checksums: 

```{r}
files = file.path(PATH, 'data/mcs/tab') 
md5s = md5sum(list.files(files, full.names=TRUE))
names(md5s) = list.files(files)
md5s
```

We then ran the script `1_3_prep_mcs.R` from [OP’s replication files](https://osf.io/e84xu/).  The resulting dataset has the following MD5 checksum:

```{r}
mcs_path = file.path(PATH, 'data/op_export/1_3_prep_mcs_data.csv')
md5s = md5sum(mcs_path)
names(md5s) = basename(mcs_path)
md5s
```

Note: we used a newer version of the MCS dataset than OP. The results generated by the OP script `2_3_sca_mcs.R` using these data (placed into `data/op_export`) differ somewhat from those provided by OP in their repository (placed into `data/op_results`).

```{r}
mcs_files = c(
  'data/op_export/2_3_sca_mcs_results_cm.rda',
  'data/op_results/2_3_sca_mcs_results_cm.rda'
)

md5s = md5sum(file.path(PATH, mcs_files))
md5s = cbind(md5s, file.info(file.path(PATH, mcs_files))[c('size')])
rownames(md5s) = mcs_files
md5s
```


### Import data

```{r}
data= read.csv(mcs_path,header=TRUE)
data= data[, !(colnames(data) %in% c('X'))] #Remove extra ID
```

### Treatment variables

The data were heavily transformed by OP. We go over the coding and potential issues by the type of variable (treatment/outcome/control).

Original MCS scales: 

- TV, games, internet, social media: 1="None", 2="0-0.5 hours", 3="0.5-1 hour", 4="1-2 hours", ..., 8=">7 hours"
- Own PC: 1="Yes", 2="No"

OP scales:

- TV, games, internet, social media: linear transformation to 1-10
- Own PC: 1="No", 10="Yes"

Since social media and internet usage are highly co-linear, we use “other internet”, with social media usage time removed (approximately), and store it in data$fcinth00rm.

```{r}
internet_vars = data[,c("fcsome00r", "fcinth00r")]
names(internet_vars) = c('Social media', 'Internet')
round(cor(internet_vars, use='pairwise.complete.obs'),3)
data$fcinth00rm = ifelse(
  data$fcinth00r - data$fcsome00r + 1 > 1,
  data$fcinth00r - data$fcsome00r + 1,
  0
)
```

This gives us the following possible treatment variables.

```{r}
x_vars= c(
  "fctvho00r", "fccomh00r", "fcsome00r", "fcinth00rm", "fccmex00r"
) 
x_names= c(
  "TV", "Electronic games", "Social media", "Other internet", "Own computer"
) 
```

As with the YRBS data, we normalize ordinary treatment values to [0, 1], which makes the regression coefficient more easily interpretable (difference between min and max usage).

```{r}
for (v in x_vars) { data[[v]]= (data[[v]] - 1)/9 }
x= data[,x_vars]; names(x)= x_names
```


### Outcome variables

Original MCS scales:

- Depressive symptoms (Mood and Feelings Questionnaire – short version; SMFQ): 1="Not true", 2="Sometimes", 3="True"
- Self-esteem (Rosenberg, positive only): 1="Strongly agree", 2="Agree", 3="Disagree", 4="Strongly disagree" (should be the opposite)
- Well-being: 1="Completely happy", ..., 7="Not happy at all"
- Strengths and Difficulties Questionnaire (SDQ) single: 1="Not true", 2="Somewhat true", 3="Certainly true"
- SDQ means [total]: 0-10 [0-40] from summing 5 [20] questions’ raw score (above - 1)

OP scales:

- Generally: linear transformation to 1-10 where 1="lowest well being", 10="highest well being"
- SDQ single: -1="lowest well being", 1="highest well being"
- SDQ prosocial mean: 0="lowest well being", 10="highest well being"
- SDQ other means: 1="lowest well being", 11="highest well being"
- SDQ total: 1="lowest well being", 41="highest well being"

```{r}
# Cohort member: 1. Depressive sympotms (Mood and Feelings Questionnaire – short version; SMFQ)
smfq_vars = c("fcmdsa00", "fcmdsb00", "fcmdsc00", "fcmdsd00", "fcmdse00", "fcmdsf00", "fcmdsg00", "fcmdsh00", "fcmdsi00", "fcmdsj00", "fcmdsk00", "fcmdsl00", "fcmdsm00")

# Cohort member: 2. self-esteem (Rosenberg)
rosenberg_vars = c("fcsati00", "fcgdql00", "fcdowl00", "fcvalu00", "fcgdsf00")

# Cohort member: 3. happiness
wellbeing_vars = c("fcscwk00", "fcwylk00", "fcfmly00", "fcfrns00", "fcschl00", "fclife00")
names(wellbeing_vars) = c("Happy with school work", "Happy with looks", "Happy with family", "Happy with friends", "Happy with school", "Happy with life")

# Parent: Strengths and Difficulties Questionnaire (SDQ)
sdq_vars = c("fconduct", "fhyper", "fpeer", "fprosoc", "femotion", "febdtot")
names(sdq_vars) = c("Parent-reported conduct problems", "Parent-reported hyperactivity/inattention", "Parent-reported peer problems", "Parent-reported prosocial", "Parent-reported emotional symptoms", "Parent-reported total difficulties")
# remove "Parent-reported" for brevity
names(sdq_vars)= sapply(names(sdq_vars), function(x) gsub('Parent-reported ', '', x), USE.NAMES = FALSE)
```

OP’s transformation of the SDQ variables is incoherent (see Supplemental Information).  We put the variables on the same 1-10 scale below. We also restore inverted measure’s meaning.

More generally, OP are using individual questions that make up established scales (SMFQ, Rosenberg, SDQ).  We calculate and use the scales (see Supplemental Information). We invert self-esteem to match the binary outcomes (1=bad).

```{r}
# Fix incorrectly coded Y variables (SDQ)
data$fprosocr = (data$fprosoc)*(9/10) + 1
data$fconductr = 9 - (data$fconduct - 1)*(9/10) + 1
data$fhyperr = 9 - (data$fhyper - 1)*(9/10) + 1
data$fpeerr = 9 - (data$fpeer - 1)*(9/10) + 1
data$femotionr = 9 - (data$femotion - 1)*(9/10) + 1
data$febdtotr = 40 - (data$febdtot - 1)*(9/40) + 1

add_r = function(s) {return(paste(s, 'r', sep=''))}
sdq_vars = sapply(sdq_vars, add_r)

# Calculate depressive sympotms score (SMFQ)
data$depression = rowSums(data[,smfq_vars] - 1, na.rm = FALSE)
data$depressionr = (data$depression)*(9/26) + 1  # normalise to 1-10

# Calculate self-esteem score (Rosenberg)
data$selfesteem = rowSums(4 - data[,rosenberg_vars], na.rm = FALSE) 
data$selfesteemr = 9 - (data$selfesteem)*(9/15) + 1  # invert and normalise to 1-10
```


Final selection of variables for linear regression: `y_main` (main), `y_pm` (extra parent) and `y_cm` (extra cohort member).

```{r}
y_main = c("depressionr", "selfesteemr", "febdtotr", "femotionr")
names(y_main) = c('Depression (adolescent)', 'Self-esteem (adolescent)', 'Total difficulties (parent)', 'Emotional problems (parent)')
y_pm = sdq_vars[1:4]
y_cm = sapply(wellbeing_vars, add_r)  # wellbeing on scale from 1-10 (generated by OP)
```

### Binary outcome variables

Outcome variables are non-binary, so we cannot run the logistical regressions used for the YRBS data. To make results more comparable, we use established cut-offs (see Supplementary Information for sources) to create binary variables and the logistical regression.

```{r}
data$depressed = data$depression >= 12  # SMFQ
data$selfesteem_lo = data$selfesteem <= 7  # Rosenberg
# SDQ
data$febdtot_hi = 41 - data$febdtot >= 14
data$femotion_hi = 11 - data$femotion >= 4
data$fconduct_hi = 11 - data$fconduct >= 3
data$fhyper_hi = 11 - data$fhyper >= 6
data$fpeer_hi = 11 - data$fpeer >= 3
data$fprosoc_lo = data$fprosoc <= 5

yvars = c('depressed', 'selfesteem_lo', 'febdtot_hi', 'femotion_hi', 'fconduct_hi', 'fhyper_hi', 'fpeer_hi', 'fprosoc_lo')
names(yvars) = c('Depressed (adolescent)', 'Low self-esteem (adolescent)', 'High total difficulties (parent)', 'High emotional problems (parent)', 'High conduct problems (parent)', 'High hyperactivity/inattention (parent)', 'High peer problems (parent)', 'Low pro-sociality (parent)')

y_cont = c('depressionr', 'selfesteemr', 'febdtotr', 'femotionr', 'fconductr', 'fhyperr', 'fpeerr', 'fprosocr')
names(y_cont) = c('Depression (adolescent)', 'Self-esteem (adolescent)', 'Total difficulties (parent)', 'Emotional problems (parent)', 'Conduct problems (parent)', 'Hyperactivity/inattention (parent)', 'Peer problems (parent)', 'Pro-sociality (parent)')
```


There exists substantial disagreement between parents and children about emotional problems!

```{r}
cor(na.omit(data[yvars]))
```


### Control variables

Control variables used in MCS study: mother's ethnicity, educational motivation (mean created by OP), employment, psychological distress (K6 Kessler scale), household income, presence of biological father, number of siblings, closeness to parents (mean created by OP), time spent with primary caretaker, long-term illnesses, adolescent's own negative attitudes towards school, primary caretaker's word activity score.

We add: sex, age, BMI (used in YRBS).

In PC’s education, OP do not convert the values correctly. Values 1-5 are NVQ levels, but 95 is “Overseas qualification” and 96 is “None of these”.  Employment is the NS-SEC 5 category (1="Manager" through 5="Routine"), which means OP exclude all unemployed parents. Instead, we create a variable "employed" that is 1 if the respondend is employed or self-employed and 0 otherwise.  Value counts are shown in the Supplemental Information.

```{r}
data$fcbmin6r= cut(data[,'fcbmin6'], breaks=c(0,20,25,30,Inf)) #discretize body mass index
levels(data$fcbmin6r)= c('underw','normal','overw','obese')

# correct PC’s education
#data[(data$fdacaq00 %in% c(95, 96)), 'fdacaq00'] = NA

# correct longstanding illness (1 = yes)
data$fpclsi00r = ifelse(data$fpclsi00 == 2, 0, data$fpclsi00)

# correct amount of time
data$fpchti00r = 5 - data$fpchti00

# create PC employment dummy
data$employed = ifelse(as.numeric(data$fdact00) %in% c(1, 2), 1, 0)

cvars = c(
  "fccsex00r", "fccage00", "fcbmin6", "edumot", 
  "fd06e00", "clpar", "fcpaab00", 
  "fpwrdscm", "employed", #"fdacaq00", "fd05s00", 
  "fpclsi00r", "fpchti00r", "fdkessl", "fdtots00", "foede000"
)
# long names
names(cvars) = c(
  "Male", "Age", "BMI", "Educational motivation",
  "Mother’s ethnicity", "Closeness to parents", "Presence of natural father", 
  "PC's word activity score", "PC employed", #"PC’s education", "PC’s employment", 
  "Longstanding illness", "Time spent with PC", "PC’s psychological distress", "Number of siblings", "Household income"
)
# short names
names(cvars) = c(
  "Male", "Age", "BMI", "Motivation",
  "Ethnicity", "Closeness", "Father", 
  "Score", "Employed",
  "Illness", "Time", "Distress", "Siblings", "Income"
)
```


## EXPLORATORY DATA ANALYSIS

### Summary statistics

Outcomes

```{r}
y_all = c(y_main, y_pm, y_cm)
y= data[, y_all]
names(y) = names(y_all)
stargazer(y, type='text')
```

Controls

```{r}
controls = data[, cvars]
names(controls) = names(cvars)
stargazer(controls, type='text')
```


### Histograms

#### Outcome variables {.tabset}

```{r, results='asis'}
for (i in 1:length(y_all)){
  cat('\n    \n')
  cat("##### ", names(y_all)[i], '\n')
  hist(x=data[, y_all[[i]]], breaks=10, main=names(y_all)[i], xlim=c(0,10), xlab=NULL)
}
```


#### Treatment variables

```{r}
for (i in 1:length(x)){
  hist(x=x[[i]], breaks=10, main=names(x)[i], xlim=c(0,1), xlab=NULL)
}
```

#### Control variables {.tabset}

```{r, results='asis'}
for (i in 1:length(cvars)){
  cat('\n    \n')
  cat("##### ", names(cvars)[i], '\n')
  hist(x=data[, cvars[[i]]], main=names(cvars)[i], xlab=NULL)
}
```


### Correlations and missing observations

Outcome variables

```{r}
y= data[,y_all]; names(y)= names(y_all)
data.frame(round(cor(y,use='pairwise.complete.obs'),3))
```


```{r}
colSums(is.na(y))
```


Treatment

Internet and social media usage are highly correlated.

```{r}
xnum= apply(x,2,as.numeric)
round(cor(xnum,use='pairwise.complete.obs'),3)
```

Control variables

```{r}
datanum= sapply(c(cvars), function(z) as.numeric(data[,z]))
colnames(datanum)= names(cvars)
round(cor(datanum, use='pairwise.complete.obs'),3)
```



## LINEARITY ANALYSIS

### Preparation

Format treatment and control variables as factors. 

```{r}

nn= names(x)
xf= x
for (i in 1:length(nn)) xf[,nn[i]]= factor(xf[,nn[i]])
nn= cvars
dataf= data
for (i in 1:length(nn)) dataf[,nn[i]]= factor(dataf[,nn[i]])

```


### Analysis {.tabset}

Linear regression with treatment variables as factors. Control variables are all numerical (e.g. test score) or have been converted in non-obvious ways (e.g. education). 

Interesting non-linearity in social media use: same association for first 5 levels (first 3 for prosociality and total difficulties), then sharp decrease in levels 6-8; has positive association with emotional symptoms. Social media use has association with peer problems and prosociality.

```{r, results='asis'}
y= data[,y_all]; names(y)= names(y_all)

idys= c(1:dim(y)[2]); idx= 1:dim(x)[2]
variables = names(x)[idx]  # which to show

for (idy in idys) {
  cat('\n    \n')
  cat("#### ", 'Outcome: ', names(y)[idy], '\n')
  
  datareg= na.omit(data.frame(y[,idy], xf[,idx], data[,c(cvars)]))
  names(datareg)[1]= 'y'
  names(datareg)[2:(1+length(idx))]= names(x)[idx]
  names(datareg)[-(1:(1+length(idx)))]= names(cvars)
  fit1= glm(y ~ ., data=datareg)
  bmle= getci(fit1)
  cat.asis(summary(fit1))
  
  for (v in variables) {
    sel= grep(v,rownames(bmle))
    plot(bmle[sel,1],ylim=range(bmle[sel,]), xaxt='n')
    segments(x0=1:length(sel),y0=bmle[sel,2],y1=bmle[sel,3])
    axis(side=1, at=1:length(sel), labels=rownames(bmle)[sel])
    title(v)
  }
}
```



## LINEAR ANALYSIS

Original outcome variables are not binary.  Use linear regression.

Run both MLE and Baysian model selection for each outcome. Model selection is done with one outcome, all treatment and all control variables. This gives some 130k models – enumeration using `modelSelectionGLM()` is not feasible. Instead, we use `modelSelection()` and Gibbs sampling with Zellner’s coefficient prior ($τ=n$ $⇒$ unit information) and a beta-binomial model prior (making the model probabilities roughly equal to the EBIC).

### Regressions {.tabset}

Weekday electronic games has negative effect on all outcomes.  Social media hours has positive effect on peer problems, prosocial behaviour and total difficulties.  The treatments generally have little/no impact on conduct problems and hyperactivity (which may have more to do with other factors and are, arguably, poor measures of well-being).

Opposite association of internet/social media usage with depression, compared to "emotional symptoms" assessment by parents. We find this result *when only one variable is included*, otherwise social media is significant/included in parents (emotional symptoms) whereas internet is significant/included in cohort member models.  Electronic games, which generally had a negative effect in parent’s assessments, has little effect.

```{r, results='asis'}
y= data[,y_all]; names(y)= names(y_all)

idys= c(1:dim(y)[2])
idx= 1:dim(x)[2]
mcs_lme= list()
mcs_ms= list()

for (idy in idys) {
  datareg= na.omit(data.frame(y[,idy], x[,idx], data[,c(cvars)]))
  names(datareg)[1]= 'y'
  names(datareg)[2:(1+length(idx))]= names(x)[idx]
  names(datareg)[-(1:(1+length(idx)))]= names(cvars)
  y_name= names(y)[idy]
  
  # MLE
  mcs_lme[[y_name]]= glm(y ~ ., data=datareg)
  cat("#### ", 'Outcome: ', y_name, ' (MLE) \n')
  cat.asis(summary(mcs_lme[[y_name]]))
  
  # Model selection
  mcs_ms[[y_name]]= mombf:::modelSelection(y ~ ., data=datareg, includevars=1, priorCoef=zellnerprior(tau=dim(datareg)[1]), priorDelta==modelbbprior(1,1), verbose=FALSE)
  cat("#### ", 'Outcome: ', y_name, ' (model selection) \n\n')
  cat.asis(format(coef(mcs_ms[[y_name]]), digits=1, scientific=FALSE), pre='\n')
}
```


### BSCA Layout

Only show 50 models and decrease the size of the legend to nicely combine in the grid figure.

```{r}
maxmodels = 50
legend_size = 1
```


### Main BSCAs {.tabset}

```{r, results='asis', fig.height=3, fig.width=4} 
bsca_main_lin = list()
for (y_name in names(y_main)) {
  cat('\n    \n')
  cat("#### ", 'Outcome: ', y_name, '\n')
  single_bsca(
    mcs_ms[[y_name]], coefidx=idx+1, maxmodels=maxmodels,
    x.labels=x_names, var.labels=names(cvars), 
    legend.cex=legend_size
  )
  bsca_main_lin[[y_name]] = recordPlot()
}
```

#### Combined plot

```{r, fig.width=8, fig.height=6}
supplement$bsca_main_lin = plot_grid(plotlist=bsca_main_lin, ncol=2, labels='auto')
ggsave(
  file.path(plot_path, 'mcs_bsca_main_lin.png'), 
  supplement$bsca_main_lin,
  height=6, width=8
)
supplement$bsca_main_lin
```

### Parent BSCAs {.tabset}

```{r, results='asis', fig.height=3, fig.width=4} 
bsca_parent_lin = list()
for (y_name in names(y_pm)) {
  cat('\n    \n')
  cat("#### ", 'Outcome: ', y_name, '\n')
  single_bsca(
    mcs_ms[[y_name]], coefidx=idx+1, maxmodels=maxmodels,
    x.labels=x_names, var.labels=names(cvars), 
    legend.cex=legend_size
  )
  bsca_parent_lin[[y_name]] = recordPlot()
}

```

#### Combined plot

```{r, fig.height=6, fig.width=8}
supplement$bsca_parent_lin = plot_grid(plotlist=bsca_parent_lin, ncol=2, labels='auto')
ggsave(
  file.path(plot_path, 'mcs_bsca_parent_lin.png'), 
  supplement$bsca_parent_lin,
  height=6, width=8
)
supplement$bsca_parent_lin
```


### Adolescent BSCAs {.tabset}

```{r, results='asis', fig.height=3, fig.width=4} 
bsca_teen_lin = list()
for (y_name in names(y_cm)) {
  cat('\n    \n')
  cat("#### ", 'Outcome: ', y_name, '\n')
  single_bsca(
    mcs_ms[[y_name]], coefidx=idx+1, maxmodels=maxmodels,
    x.labels=x_names, var.labels=names(cvars),
    legend.cex=legend_size
  )
  bsca_teen_lin[[y_name]] = recordPlot()
}
```

#### Combined plot

```{r, fig.width=8, fig.height=9}
supplement$bsca_teen_lin = plot_grid(plotlist=bsca_teen_lin, ncol=2, labels='auto')
ggsave(
  file.path(plot_path, 'mcs_bsca_teen_lin.png'), 
  supplement$bsca_teen_lin,
  height=9, width=8
)
supplement$bsca_teen_lin
```


## LOGIT ANALYSIS


### Regressions

For computational efficiency, we perform the logistic regression BMA on the top 100 linear models for each outcome.

```{r}
# assumes outcome variables have the same order in mcs_ms and yvars
pp_lin = lapply(mcs_ms[seq_along(yvars)], postProb)

b = list(); ms_logit = list()
for (idy in 1:length(yvars)) {
  yvar = yvars[idy]; yname = names(yvars)[idy]
  print(yname)  
  
  datareg = na.omit(data[c(yvar, x_vars, cvars)])
  names(datareg) = c('y', x_names, names(cvars))
  reg = y ~ .
  
  # MLE
  fit1 = glm(reg, data=datareg, family=binomial(link='logit'))
  print(summary(fit1))
  
  # BMA
  models = as.matrix.pp(pp_lin[[idy]], nummodels=100, numvars=length(datareg))
  ms_logit[[yname]] = mombf:::modelSelectionGLM(reg, data=datareg, familyglm= binomial(link='logit'), priorDelta=modelbbprior(1,1), models=models)
  b[[yname]] = coef(ms_logit[[yname]])
  cat('\n')
  print(format(as.data.frame(coef(ms_logit[[yname]])), digits=1, scientific=FALSE))
}
```


### Specification curves

```{r, results='asis', fig.height=3, fig.width=4} 
y_labels = round(c(1/2, 1, 2, 3, 4), 2); names(y_labels) = log(y_labels)  # y scale as odds ratio

bsca_logit = list()
for (idy in 1:length(yvars)) {
  yvar = yvars[idy]; yname = names(yvars)[idy]
  cat("\n#### ", 'Outcome: ', yname, '\n')
  single_bsca(
    ms_logit[[yname]], coefidx=idx+1, maxmodels=maxmodels,
    x.labels=x_names, var.labels=names(cvars), y.labels=y_labels,
    legend.cex=legend_size
  )
  bsca_logit[[yname]] = recordPlot()
}

```

#### Combined main

```{r, fig.width=8, fig.height=6}
supplement$bsca_logit = plot_grid(plotlist=bsca_logit[1:4], ncol=2, labels='auto')
ggsave(
  file.path(plot_path, 'mcs_bsca_logit.png'), 
  supplement$bsca_logit,
  height=6, width=8
)
supplement$bsca_logit
```

#### Combined parent

```{r, fig.width=8, fig.height=6}
supplement$bsca_parent_logit = plot_grid(plotlist=bsca_logit[5:8], ncol=2, labels='auto')
ggsave(
  file.path(plot_path, 'mcs_bsca_parent_logit.png'), 
  supplement$bsca_parent_logit,
  height=6, width=8
)
supplement$bsca_parent_logit
```


### Using full model space

For computational efficiency, we used the most likely linear models above.  The results go through if sampling from the full model space, as shown for one outcome below.

Results for high total difficulties above:
```{r}
single_bsca(ms_logit$`High total difficulties (parent)`, coefidx=2:6)
```

Results for full model space:
```{r}
yvar = yvars[3]
datareg = na.omit(data[c(yvar, x_vars, cvars)])
names(datareg) = c('y', x_names, names(cvars))
fit_bas <- bas.glm(
  y~., data=datareg, family=binomial(link='logit'), 
  betaprior=bic.prior(), modelprior=beta.binomial(),
  method='MCMC', n.models=200
)
```

```{r}
single_bsca(fit_bas, coefidx=2:6)
```


## SUBGROUP ANALYSIS

### Logit

We perform a gender subgroup analysis. The variables are parameterized as described in the paper. Interaction terms are included only if the corresponding main variable is included. BMA is done through Gibbs sampling.

```{r}
idg="Male"
{
  options(na.action='na.pass')
  x.reg = model.matrix(~ ., x[,idx])[, -1]
  colnames(x.reg) = x_names
}
ms_inter = list(); coef_inter = list()

for (idy in 1:length(yvars)) {
  yname = names(yvars)[idy]
  message(yname)
  
  datareg = na.omit(data[c(yvars[idy], x_vars, cvars)])
  names(datareg) = c('y', x_names, names(cvars))
  
  # treatments
  idt = colnames(x.reg)
  datareg[idt] = datareg[idt] - 1/2
  # group
  g = datareg[[idg]]; rho = sum(g == 1)/length(g)
  datareg[[idg]] = ifelse(g == 1, 1-rho, -rho)
  # interaction 
  inter = datareg[idt]*datareg[[idg]]
  colnames(inter) = paste0(colnames(inter), " × male")
  
  d = mombf:::createDesign(y ~ ., cbind(datareg, inter))
  p=length(d$constraints); pi=ncol(inter)
  d$constraints[(p-pi+1):p] = 1:pi+1
  ms_inter[[yname]] = mombf:::modelSelection(
    d$y, d$x, includevars=1, family="binomial", 
    priorDelta=modelbbprior(1,1), priorCoef=bicprior(), priorGroup=bicprior(),
    niter=NITER_MCS, groups=d$groups, constraints=d$constraints,
  )
  coef_inter[[yname]] = coef(ms_inter[[yname]])
}
```

The gender effect is always zero.

```{r}
supplement$mbsca_inter = multi_bsca(
  coef_inter, ms=ms_inter, conversion=exp,  y.scale='Odds ratio', 
  treatments=tail(rownames(coef_inter$`Depressed (adolescent)`), n=length(x_names)),
  extract.color=TRUE
) + geom_hline(yintercept=1, lwd=0.2) + ylim(0.8, 1.5)
supplement$mbsca_inter
```

### Raw data

Yet, there is a higher correlation between social media use and self-reported regression for females than males.

```{r}
plotdf = data[!is.na(data$depressed), ] %>% 
  rename(SocialMediaUsage="fcsome00r", Male="fccsex00r") %>%
  mutate_at("Male", as.logical) %>%
  mutate_at("depressed", as.numeric)

# plotdf %>% 
#   select(Male, SocialMediaUsage, depressed) %>% 
#   drop_na() %>% 
#   group_by(Male) %>% 
#   summarise(cor=cor(SocialMediaUsage, depressed, method="spearman"))

plotdf %>%
  group_by(Male, SocialMediaUsage) %>%
  summarise(depressed = mean(depressed)) %>%
  ggplot(aes(x=SocialMediaUsage, y=depressed, color=Male)) +
  geom_col(position=position_dodge())
```

This correlation is even stronger when using the continuous outcome variable. 

```{r}
# plotdf %>% 
#   select(Male, SocialMediaUsage, depression) %>% 
#   drop_na() %>% 
#   group_by(Male) %>% 
#   summarise(cor=cor(SocialMediaUsage, depression, method="spearman"))

plotdf %>%
  group_by(Male, SocialMediaUsage) %>%
  summarise(depression = mean(depression)) %>%
  ggplot(aes(x=SocialMediaUsage, y=depression, color=Male)) +
  geom_col(position=position_dodge())
```


### Linear

We therefore also run the subgroup analysis with the original variables (normalized to 1-10) and a linear model. Here we find a gender effect for depression, in a simple GLM regression. We perform a BMA analysis on all outcomes in the main analysis file.

```{r}
idg="Male"
datareg = na.omit(data[c("depression", x_vars, cvars)])
datareg$depression = datareg$depression/max(datareg$depression)
names(datareg) = c('y', x_names, names(cvars))

# treatments
idt = x_names
datareg[idt] = datareg[idt] - 1/2
# group
g = datareg[[idg]]; rho = sum(g == 1)/length(g)
datareg[[idg]] = ifelse(g == 1, 1-rho, -rho)
# interaction 
inter = datareg[idt]*datareg[[idg]]
colnames(inter) = paste0(colnames(inter), ":male")

fit = lm(y ~ ., data=c(datareg, inter))
summary(fit)
```


## SAVE DATA

Save the data for use in `bsca_main.Rmd`.

```{r}
save(
  data, yvars, y_cont, x_vars, x_names, cvars, pp_lin, supplement,
  file=file.path(PATH, 'data/export/mcs.Rdata')
)
```



